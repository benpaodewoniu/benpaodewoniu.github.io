<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"benpaodewoniu.github.io","root":"/","scheme":"Pisces","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":true},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="github 说道回归，我们一般指的是线性回归，线性回归的一般理论是：">
<meta property="og:type" content="article">
<meta property="og:title" content="预测数值型数据：回归">
<meta property="og:url" content="http://benpaodewoniu.github.io/2018/06/14/machinelearning8/index.html">
<meta property="og:site_name" content="犀牛的博客">
<meta property="og:description" content="github 说道回归，我们一般指的是线性回归，线性回归的一般理论是：">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_0.JPG">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_1.JPG">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_2.JPG">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_0.JPG">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_3.jpg">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_4.jpg">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_5.JPG">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_6.JPG">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_7.jpg">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_8.jpg">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_9.JPG">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_9.JPG">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_10.JPG">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_11.jpg">
<meta property="og:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_12.jpg">
<meta property="article:published_time" content="2018-06-14T08:56:34.000Z">
<meta property="article:modified_time" content="2021-01-22T06:14:22.000Z">
<meta property="article:author" content="犀牛">
<meta property="article:tag" content="machine learning">
<meta property="article:tag" content="回归">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://benpaodewoniu.github.io/images/machinelearningalgorithm/2_0.JPG">

<link rel="canonical" href="http://benpaodewoniu.github.io/2018/06/14/machinelearning8/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true
  };
</script>

  <title>预测数值型数据：回归 | 犀牛的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">犀牛的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">姑苏城外一茅屋，万树梅花月满天</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-help">

    <a href="/help/" rel="section"><i class="fa fa-fw fa-question"></i>指引</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-invest">

    <a href="/invest/" rel="section"><i class="fa fa-fw fa-book"></i>投资</a>

  </li>
        <li class="menu-item menu-item-links">

    <a href="/links/" rel="section"><i class="fa fa-fw fa-link"></i>友链</a>

  </li>
        <li class="menu-item menu-item-books">

    <a href="/books/" rel="section"><i class="fa fa-fw fa-book"></i>书籍</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>地图</a>

  </li>
        <li class="menu-item menu-item-problem">

    <a href="/problem/" rel="section"><i class="fa fa-fw fa-question"></i>问前需看</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://benpaodewoniu.github.io/2018/06/14/machinelearning8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/icon/icon.png">
      <meta itemprop="name" content="犀牛">
      <meta itemprop="description" content="求职web3工作，欢迎联系 xiniublog@163.com">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="犀牛的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          预测数值型数据：回归
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2018-06-14 16:56:34" itemprop="dateCreated datePublished" datetime="2018-06-14T16:56:34+08:00">2018-06-14</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-01-22 14:14:22" itemprop="dateModified" datetime="2021-01-22T14:14:22+08:00">2021-01-22</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/" itemprop="url" rel="index"><span itemprop="name">人工智能</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/" itemprop="url" rel="index"><span itemprop="name">线性回归</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2018/06/14/machinelearning8/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2018/06/14/machinelearning8/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a href="https://github.com/benpaodewoniu/Machine-Learning-in-Action">github</a></p>
<p>说道回归，我们一般指的是线性回归，线性回归的一般理论是：</p>
<a id="more"></a>

<p>给你一个输入值，然后有一个计算公式，从而由输入值转变为输出值。</p>
<p>在看这个算法之前，你需要了解最小二乘法，可以参考我以前的博文。</p>
<p><a href="/2018/06/17/math4/">最小二乘法</a></p>
<br/>

<h1 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h1><br/>

<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>线性回归理论认为，输入项分别乘以一些常量，再将结果加起来，就能得到输出值，比如：</p>
<pre><code>housePrice = area * 0.1 + years * 0.02 + 。。。</code></pre><p>但是也有非线性回归理论，他们的认为输入值之间都是有联系的：</p>
<pre><code>housePrice = ( area / years ) * 0.1</code></pre><p>但是，我们探究的皆是线性回归。</p>
<h2 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h2><p>假设所有的常量都在 W 向量中，所有的每一个输入项都在 Xi 向量中(N * 1)，则对于一个样例来说：</p>
<pre><code>#octave语法
Y1 = X1&apos; * W</code></pre><p>但是，算法的关键就是如何计算出 W 向量。</p>
<p>事实上，我们一开始对 W 向量都是随机初始化，当然初始化的方案有很多。</p>
<p>但是随机初始化的结果肯定不是最终的 W ，所以，在这里我们引入误差，最常用的是平方误差。</p>
<p>再次再次重申一下各符号的意义 Y 是存储标签的向量 X 是存储输入项的矩阵（它的每一行代表一个样例） W 是常量的向量 J 是误差值。</p>
<p>这里有一个注意点，首先 X 是存储所有输入项的矩阵，但是 Xi 是存储单例的向量，其形式是 N * 1。</p>
<pre><code>#octave语法
J = ∑ (Y- Xi&apos; * W)^2 [Xi 是一个向量 ，i = 1 ... ... N]
#用矩阵表示
J = (Y - X * W)&apos; * (Y - X * W)</code></pre><p>因为 J 是一个平方函数，所以自然而然我们就能想到，其图像是一个碗形，那么最优点必然是 J 最小的点，也就是碗底，那么只要用办法接近碗底那么我们就能计算出 w。</p>
<p>一般，计算 w 我们有两种方法，一是直接求（直接求不需要借助平方误差），二是用梯度下降。</p>
<h3 id="直接求-W"><a href="#直接求-W" class="headerlink" title="直接求 W"></a>直接求 W</h3><p>考虑下面的线性代数：</p>
<pre><code>#octave语法 pinv(A)是求逆
Y = X * W
pinv(X) * Y = pinv(X) * X * W
pinv(X) * Y = W
pinv(X) * pinv(X&apos;) * X&apos; * Y = W
pinv(X&apos; * X) * X&apos; * Y = W</code></pre><p>在书中的最终式子如图所示：</p>
<p><img src="/images/machinelearningalgorithm/2_0.JPG" alt=""></p>
<p>看到上面的式子，你大概有两个疑问。</p>
<ol>
<li>为什么会有导数第二段那么多余的式子？</li>
</ol>
<p>因为正确的推导公式是求导求出来的，不过，我对那个推导过程很有疑虑，所以最后只能用验证的方法求出书中的式子。</p>
<ol start="2">
<li>为什么不直接 W = pinv(X) * Y 还非得多加一个，成为 pinv(X’ * X) * X’ * Y = W？</li>
</ol>
<p>如图：</p>
<p><img src="/images/machinelearningalgorithm/2_1.JPG" alt=""></p>
<p>因为本来按照求导得出来的式子，正好和第一个图的式子一样，至于为什么不化成更简单的式子，其实很简单。</p>
<p>因为我们不知道下面图片是否有逆。</p>
<p><img src="/images/machinelearningalgorithm/2_2.JPG" alt=""></p>
<p>假设，X 是 m * n 其中 m &lt; n</p>
<pre><code>R（X） &lt;= m
而图片中的式子，其形式 = n * n
又因为上面式子的秩必然 &lt;= m
所以这种情况没有逆</code></pre><p>以上就是直接求的原理。其最终式子，我在这里再贴一下：</p>
<p><img src="/images/machinelearningalgorithm/2_0.JPG" alt=""></p>
<h3 id="用梯度下降的方式求-W"><a href="#用梯度下降的方式求-W" class="headerlink" title="用梯度下降的方式求 W"></a>用梯度下降的方式求 W</h3><br/>

<h1 id="代码示例"><a href="#代码示例" class="headerlink" title="代码示例"></a>代码示例</h1><br/>

<h2 id="直接求-W-1"><a href="#直接求-W-1" class="headerlink" title="直接求 W"></a>直接求 W</h2><h3 id="代码编辑"><a href="#代码编辑" class="headerlink" title="代码编辑"></a>代码编辑</h3><p>首先你可以在我的github中看见相关的数据。我用的是 ex0.txt。</p>
<p>我用 octave 画出 其中的数据分布，如图：</p>
<p><img src="/images/machinelearningalgorithm/2_3.jpg" alt=""></p>
<p>而我们的目的就是拟合出一条线，使得误差 J 最小，我们决定使用直接求的方法。参见下述代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(filename)</span>:</span></span><br><span class="line">    numFeat = len(open(filename).readline().split(<span class="string">'\t'</span>)) - <span class="number">1</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = []</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeat):</span><br><span class="line">            lineArr.append(float(curLine[i]))</span><br><span class="line">        dataMat.append(lineArr)</span><br><span class="line">        labelMat.append(float(curLine[<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">standRegression</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    xMat = np.mat(x)</span><br><span class="line">    yMat = np.mat(y).T</span><br><span class="line">    xTx = xMat.T * xMat</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        print(<span class="string">'不可逆'</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = xTx.I * (xMat.T * yMat)</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"></span><br><span class="line">x,y = loadData(<span class="string">'ex0.txt'</span>)</span><br><span class="line">print(standRegression(x,y))</span><br><span class="line">    <span class="comment">#[[3.00774324]</span></span><br><span class="line">    <span class="comment"># [1.69532264]]</span></span><br></pre></td></tr></table></figure>

<p>根据上得到的 W，用 octave 绘图：</p>
<p><img src="/images/machinelearningalgorithm/2_4.jpg" alt=""></p>
<h3 id="注意点"><a href="#注意点" class="headerlink" title="注意点"></a>注意点</h3><p>我们再次观察 ex0.txt 中的数据：</p>
<pre><code>1.000000    0.067732    3.176513</code></pre><p>第一个是 1，第二个是输入项，第三个是标签，那么为什么第一个数值总是 1 。其实， 1 代表的是偏差。</p>
<p>我们都知道直线的方程如下形式：</p>
<pre><code>y = k * x + b</code></pre><p>那个 b 就是偏差。所以，我们的第一个数值为 1 ，就是要求偏差 b。</p>
<br/>

<h1 id="欠拟合"><a href="#欠拟合" class="headerlink" title="欠拟合"></a>欠拟合</h1><br/>

<p>在这里我不在讲述欠拟合的定义，关于欠拟合的解决方案，这里是局部加权线性回归方法(LWLR)。</p>
<h2 id="LWLR"><a href="#LWLR" class="headerlink" title="LWLR"></a>LWLR</h2><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>在该算法中，我们给待测点附近的每一点赋予一定的权重。什么叫赋予权重呢？我们考虑之前的计算方式，最终 W 的计算数值是根据所有样本得到的，讲究的是一个大致的平均，最终得到的是一条直线。</p>
<p>但是，我们知道对于单一样本来说离它近的点和它相似，离它远的点和它不同，所以对于单一样本点如果采用全部样本对它进行归纳的话不免会失去它的特征，但是如果对某一单一样本之外的点赋予权重，比如离它近的权重大，离它远的权重小，那么，最后就可以对这个单一样本点得到独立的 Wi。</p>
<p>最终，每一个样本点都有一个相应的 Wi ，所以最后的表现形式是曲线。</p>
<p>在这里要说明的是，当所有样本点对单一样本贡献一样的时候，最后会是一条直线，也就是和原来一样，产生欠拟合的效果。</p>
<p>当样本对单一样本的权重不一样的时候，离得越近权重越大，是最合适的情况。</p>
<p>但是，当只有最近的样本才对权重有贡献，甚至极端一点，只有自身才对自己有贡献的时候，会产生过拟合的效果。</p>
<p>那我们应当如何选权重呢？</p>
<p>首先给出计算 W 的新式子：</p>
<p><img src="/images/machinelearningalgorithm/2_5.JPG" alt=""></p>
<p>而上面式子中的矩阵 A 就是用来给每一个数据赋予权重。</p>
<p>LWLR使用核概念（和支持向量机的核概念一样）。核有很多，最常见的就是高斯核，高斯核对应的权重如下：</p>
<p><img src="/images/machinelearningalgorithm/2_6.JPG" alt=""></p>
<p>观看如下图片：</p>
<p><img src="/images/machinelearningalgorithm/2_7.jpg" alt=""></p>
<p>假设我们要观测的点是1.5，这就是不同的 K 的取值，其大约有多少样本点对样本点 1.5 的贡献。可以看出 K 越小，越少的样本点才有权重。</p>
<p>当 K = 1 的时候，所有的样本点对单一样本的贡献权重一样，会出现欠拟合状态，如果 K 的值非常小，那么会出现过拟合的状态。</p>
<h3 id="看代码解释"><a href="#看代码解释" class="headerlink" title="看代码解释"></a>看代码解释</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(filename)</span>:</span></span><br><span class="line">    numFeat = len(open(filename).readline().split(<span class="string">'\t'</span>)) - <span class="number">1</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = []</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeat):</span><br><span class="line">            lineArr.append(float(curLine[i]))</span><br><span class="line">        dataMat.append(lineArr)</span><br><span class="line">        labelMat.append(float(curLine[<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlr</span><span class="params">(testPoint,xArr,yArr,k)</span>:</span></span><br><span class="line">    xMat = np.mat(xArr)</span><br><span class="line">    yMat = np.mat(yArr).T</span><br><span class="line">    m    = np.shape(xMat)[<span class="number">0</span>]</span><br><span class="line">    weighs = np.mat(np.eye(m))</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(m):</span><br><span class="line">        diffMat = testPoint - xMat[j,:]</span><br><span class="line">        weighs[j,j] = np.exp(diffMat * diffMat.T/(<span class="number">-2.0</span> * k ** <span class="number">2</span>))</span><br><span class="line">    xTx = xMat.T * (weighs * xMat)</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(xTx) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = xTx.I * (xMat.T * (weighs * yMat))</span><br><span class="line">    <span class="keyword">return</span> testPoint * ws                          <span class="comment">#返回的是预测的 y 值</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">lwlrTest</span><span class="params">(testArr,xArr,yArr,k)</span>:</span></span><br><span class="line">    m = np.shape(testArr)[<span class="number">0</span>]</span><br><span class="line">    yHat = np.zeros(m)</span><br><span class="line">    y = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(m):</span><br><span class="line">        yHat[i] = lwlr(testArr[i],xArr,yArr,k)</span><br><span class="line">        y[yHat[i]] = testArr[i][<span class="number">1</span>]</span><br><span class="line">    y = sorted(y.items(), key=operator.itemgetter(<span class="number">1</span>), reverse=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">return</span> yHat,y</span><br><span class="line"></span><br><span class="line">data,label = loadData(<span class="string">'ex0.txt'</span>)</span><br><span class="line">yHat,y = lwlrTest(data,data,label,<span class="number">0.003</span>)</span><br><span class="line">file = open(<span class="string">'test.txt'</span>,<span class="string">'w'</span>)</span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> y:</span><br><span class="line">    file.writelines(str(f[<span class="number">0</span>]) + <span class="string">'\t'</span> + str(f[<span class="number">1</span>]))</span><br><span class="line">    file.write(<span class="string">'\n'</span>)</span><br><span class="line">file.close()</span><br></pre></td></tr></table></figure>

<h3 id="图像"><a href="#图像" class="headerlink" title="图像"></a>图像</h3><p><img src="/images/machinelearningalgorithm/2_8.jpg" alt=""></p>
<p>我们从图上也可以清晰的看出当 k  = 0.01 的时候拟合程度是最好的。</p>
<p>在 github 中我还放置了预测鲍鱼年龄的数据（abalone.txt），有兴趣的可以去玩一下。</p>
<br/>

<h1 id="缩减系数来理解数据"><a href="#缩减系数来理解数据" class="headerlink" title="缩减系数来理解数据"></a>缩减系数来理解数据</h1><br/>

<h2 id="什么是缩减系数？"><a href="#什么是缩减系数？" class="headerlink" title="什么是缩减系数？"></a>什么是缩减系数？</h2><p>通过对损失函数(即优化目标)加入惩罚项，使得训练求解参数过程中会考虑到系数的大小，通过设置缩减系数(惩罚系数)，会使得影响较小的特征的系数衰减到0，只保留重要的特征。常用的缩减系数方法有lasso(L1正则化)，岭回归(L2正则化)。</p>
<p>什么是缩减？</p>
<pre><code>通过引入 +λI 来限制所有 W 之和，通过引入惩罚项，能够减少不重要的参数，这个技术在统计学中叫做缩减。</code></pre><h2 id="缩减系数的目的"><a href="#缩减系数的目的" class="headerlink" title="缩减系数的目的"></a>缩减系数的目的</h2><h3 id="消除噪声特征"><a href="#消除噪声特征" class="headerlink" title="消除噪声特征:"></a>消除噪声特征:</h3><pre><code>如果模型考虑了一些不必要的特征，那么这些特征就算是噪声。噪声是没必要的，使得模型复杂，降低模型准确性，需要剔除。</code></pre><h3 id="消除关联的特征"><a href="#消除关联的特征" class="headerlink" title="消除关联的特征"></a>消除关联的特征</h3><pre><code>如果模型的特征空间中存在关联的特征，这会使得模型不适定，即模型参数会有多解。训练得到的只是其中一个解，这个解往往不能反映模型的真实情况，会误导模型的分析与理解。训练求解的模型参数受样本影响特别大，样本变化一点点，参数解就跳到另一组解去了。总之，模型是不稳定的。</code></pre><h2 id="岭回归（ridge-regression）"><a href="#岭回归（ridge-regression）" class="headerlink" title="岭回归（ridge regression）"></a>岭回归（ridge regression）</h2><p>在上述的 LWLR 中，我们很容易知道这个算法的缺点，它对每一个单独样本计算权重的时候，都需要整合所有的数据，假设数据量非常大，那么这个算法的时间就会比较长。</p>
<p>而且，我们知道， LWLR 的前提是下面的式子逆矩阵存在：</p>
<p><img src="/images/machinelearningalgorithm/2_9.JPG" alt=""></p>
<p>我们考虑这样一种情况，就是我们的特征比样本还多，也就是说输入矩阵 X 不是满秩矩阵（奇异矩阵）。</p>
<p>满秩矩阵</p>
<pre><code>设A是n阶矩阵, 若r（A） = n, 则称A为满秩矩阵。但满秩不局限于n阶矩阵。
若矩阵秩等于行数，称为行满秩；若矩阵秩等于列数，称为列满秩。
既是行满秩又是列满秩则为n阶矩阵即n阶方阵。
行满秩矩阵就是行向量线性无关，列满秩矩阵就是列向量线性无关；所以如果是方阵,行满秩矩阵与列满秩矩阵是等价的。</code></pre><p>矩阵的定义</p>
<pre><code>由 m × n 个数a排成的m行n列的数表称为m行n列的矩阵，简称m × n矩阵。</code></pre><p>X 不是满秩矩阵，在计算下面式子的时候就会出错。</p>
<p><img src="/images/machinelearningalgorithm/2_9.JPG" alt=""></p>
<p>而为了让上述式子成立，统计学家引入岭回归的概念。</p>
<p>岭回归就是在矩阵 X’X 上加一个 λI 从而使矩阵变成非奇异矩阵。进而对 X’X + λI 求逆。</p>
<p>所以回归公式变成：</p>
<p><img src="/images/machinelearningalgorithm/2_10.JPG" alt=""></p>
<p>上面式子 λ 是一个常量，当 λ 等于 0 的时候，原式就退变成为最小二乘法， I 是单位矩阵。为什么说为岭呢？正是因为 I 是单位矩阵，其对角线有数，其它的为零，所以像山岭。</p>
<p>岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价获得回归系数更为符合实际、更可靠的回归方法，对病态数据的拟合要强于最小二乘法。</p>
<p>对于有些矩阵，矩阵中某个元素的一个很小的变动，会引起最后计算结果误差很大，这种矩阵称为“病态矩阵”。有些时候不正确的计算方法也会使一个正常的矩阵在运算中表现出病态。对于高斯消去法来说，如果主元（即对角线上的元素）上的元素很小，在计算时就会表现出病态的特征。</p>
<p>再加上 λI 后，其变成奇异矩阵的概率下降。</p>
<h3 id="代码示例-1"><a href="#代码示例-1" class="headerlink" title="代码示例"></a>代码示例</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(filename)</span>:</span></span><br><span class="line">    numFeat = len(open(filename).readline().split(<span class="string">'\t'</span>)) - <span class="number">1</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = []</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeat):</span><br><span class="line">            lineArr.append(float(curLine[i]))</span><br><span class="line">        dataMat.append(lineArr)</span><br><span class="line">        labelMat.append(float(curLine[<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeRegress</span><span class="params">(xMat,yMat,lam)</span>:</span></span><br><span class="line">    xTx = xMat.T * xMat</span><br><span class="line">    denom = xTx + np.eye(np.shape(xMat)[<span class="number">1</span>]) * lam</span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(denom) == <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    ws = denom.I * (xMat.T * yMat)</span><br><span class="line">    <span class="keyword">return</span> ws</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ridgeTest</span><span class="params">(xArr,yArr)</span>:</span></span><br><span class="line">    xMat = np.mat(xArr)</span><br><span class="line">    yMat = np.mat(yArr).T</span><br><span class="line">    yMean = np.mean(yMat,<span class="number">0</span>)</span><br><span class="line">    yMat = yMat - yMean</span><br><span class="line">    xMean = np.mean(xMat,<span class="number">0</span>)</span><br><span class="line">    xVar = np.var(xMat,<span class="number">0</span>)</span><br><span class="line">    numTest = <span class="number">30</span></span><br><span class="line">    wMat = np.zeros((numTest,np.shape(xMat)[<span class="number">1</span>]))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTest):</span><br><span class="line">        ws = ridgeRegress(xMat,yMat,np.exp(i - <span class="number">10</span>))</span><br><span class="line">        wMat[i,:] = ws.T</span><br><span class="line">    <span class="keyword">return</span> wMat</span><br><span class="line"></span><br><span class="line">abX,abY = loadData(<span class="string">'abalone.txt'</span>)</span><br><span class="line">rideg = ridgeTest(abX,abY)</span><br></pre></td></tr></table></figure>

<p>使用岭回归和缩减技术需要对特征进行标准化处理。</p>
<p>在上面代码的 ridgeTest() 中有数据归一化的应用，关于数据归一化，你可以参考我的博文。</p>
<p><a href="/2018/06/17/machinelearning1/">机器学习中的数据归一</a></p>
<p>最后画出的图如下：</p>
<p><img src="/images/machinelearningalgorithm/2_11.jpg" alt=""></p>
<p>abalone.txt 中的一个样例</p>
<pre><code>1    0.455    0.365    0.095    0.514    0.2245    0.101    0.15    15</code></pre><p>为八个特征。</p>
<p>关于图像的解读，最后那个 ridge 是 30 * 8，行数分别代表不同的 lam ，列数代表每一个 lam 对应的 Wi。</p>
<p>每一个平行于 Y 轴的直线上的焦点对应于一个 Wi。</p>
<p>但是为什么只有八条曲线，是因为我们按照列的方式来计算的。横坐标代表 log(λ) 纵坐标代表那个特定的 λ 对应的 W。</p>
<p>从图上我们可知，假如 λ 非常小的时候，图像左边，那么 + λI 相当于不加，所以最后得到的 Wi 相当于普通回归一样。</p>
<p>假如 λ 非常大，图像右边，那个 + λI 确实会使得原来的特征权重降低，具体原理可以参考正则化去过拟合原理，最终导致 Wi 缩减为零。</p>
<p>所以，最好的 Wi 是取中间某一条直线上的 Wi 数值。但是具体取哪个需要用测试数据来敲定。</p>
<h2 id="前向逐步回归"><a href="#前向逐步回归" class="headerlink" title="前向逐步回归"></a>前向逐步回归</h2><p>说实话，当我看见《机器学习实战》中的前向逐步回归算法感到很吃惊，哇塞，这个算法也太简陋了吧。</p>
<p>后来想想也就理解了，前向逐步回归说的是一种思想，并不是什么算法，所以前向逐步回归可以用贪心算法也可以用梯度下降算法。</p>
<h3 id="前向逐步回归——贪心算法"><a href="#前向逐步回归——贪心算法" class="headerlink" title="前向逐步回归——贪心算法"></a>前向逐步回归——贪心算法</h3><p>前向逐步回归的原理是每一步进可能减少误差，一开始，所有的 W 都设为 1 。然后每一步的策略就是增加或减少 W 使得误差 J 减小。</p>
<p>所谓的贪心算法只是很笨的一种方法，如果真的要使用前向逐步回归，应该选用更好的算法。但是，在这里还是写一下贪心算法的代码吧。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadData</span><span class="params">(filename)</span>:</span></span><br><span class="line">    numFeat = len(open(filename).readline().split(<span class="string">'\t'</span>)) - <span class="number">1</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    labelMat = []</span><br><span class="line">    fr = open(filename)</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr = []</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(numFeat):</span><br><span class="line">            lineArr.append(float(curLine[i]))</span><br><span class="line">        dataMat.append(lineArr)</span><br><span class="line">        labelMat.append(float(curLine[<span class="number">-1</span>]))</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rssError</span><span class="params">(yArr,yHatArr)</span>:</span> <span class="comment">#yArr and yHatArr both need to be arrays</span></span><br><span class="line">    <span class="keyword">return</span> ((yArr-yHatArr)**<span class="number">2</span>).sum()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">regularize</span><span class="params">(xMat)</span>:</span> <span class="comment"># 数据归一化</span></span><br><span class="line">    inMat = xMat.copy()</span><br><span class="line">    inMeans = np.mean(inMat,<span class="number">0</span>)</span><br><span class="line">    inVar = np.var(inMat,<span class="number">0</span>)</span><br><span class="line">    inMat = (inMat - inMeans)/inVar</span><br><span class="line">    <span class="keyword">return</span> inMat</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">greed</span><span class="params">(xArr,yArr,eps=<span class="number">0.01</span>,num=<span class="number">100</span>)</span>:</span></span><br><span class="line">    xMat = np.mat(xArr)</span><br><span class="line">    yMat = np.mat(yArr).T</span><br><span class="line">    yMean = np.mean(yMat,<span class="number">0</span>)</span><br><span class="line">    xMat = regularize(xMat)</span><br><span class="line">    m,n = np.shape(xMat)</span><br><span class="line">    returnMat = np.zeros((num,n))</span><br><span class="line">    ws = np.zeros((n,<span class="number">1</span>))</span><br><span class="line">    wsTest = ws.copy()</span><br><span class="line">    wsMax = ws.copy()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num):</span><br><span class="line">        print(ws.T)</span><br><span class="line">        lowError = math.inf <span class="comment"># 表示正无穷</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">            <span class="keyword">for</span> sign <span class="keyword">in</span> [<span class="number">-1</span>,<span class="number">1</span>]:</span><br><span class="line">                wsTest = ws.copy()</span><br><span class="line">                wsTest[j] += eps * sign</span><br><span class="line">                yTest = xMat * wsTest</span><br><span class="line">                rssE = rssError(yMat.A,yTest.A)</span><br><span class="line">                <span class="keyword">if</span> rssE &lt; lowError:</span><br><span class="line">                    lowError = rssE</span><br><span class="line">                    wsMax = wsTest</span><br><span class="line">        ws = wsMax.copy()</span><br><span class="line">        returnMat[i,:] = ws.T</span><br><span class="line">    <span class="keyword">return</span> returnMat</span><br><span class="line"></span><br><span class="line">x,y = loadData(<span class="string">'abalone.txt'</span>)</span><br><span class="line">print(greed(x,y,<span class="number">0.01</span>,<span class="number">200</span>))</span><br><span class="line">	<span class="comment">#[[ 0.    0.    0.   ...  0.    0.    0.  ]</span></span><br><span class="line">	<span class="comment"># [ 0.    0.    0.   ...  0.    0.    0.  ]</span></span><br><span class="line">	<span class="comment"># [ 0.    0.    0.   ...  0.    0.    0.  ]</span></span><br><span class="line">	<span class="comment"># 。。。</span></span><br><span class="line">	<span class="comment"># [ 0.05  0.    0.09 ... -0.64  0.    0.36]</span></span><br><span class="line">	<span class="comment"># [ 0.04  0.    0.09 ... -0.64  0.    0.36]</span></span><br><span class="line">	<span class="comment"># [ 0.05  0.    0.09 ... -0.64  0.    0.36]]</span></span><br><span class="line">	<span class="comment"># 如果用最小二乘法计算，当然这个最小二乘法是直接求的最小二乘法，还有里面的数据也是和 greedy 一样进行了数据归一化处理。</span></span><br><span class="line">	<span class="comment"># 其最终输出如下：</span></span><br><span class="line">	<span class="comment"># [[ 0.0430442  -0.02274163  0.13214087  0.02075182  2.22403814 -0.99895312 -0.11725427  0.16622915]]</span></span><br></pre></td></tr></table></figure>

<p>在这里我可能对要产生的疑点进行解答。</p>
<pre><code>1.为什么 greedy 最后输出是不相等的数值？
因为我们一共是三层循环，在第二层循环中我们是对每一个 Wi 进行赋值，所以最后不相等。
2.为什么最终的 W1 和 W6 都是 0？
这是因为这些特征对于贪心算法最后的所得的 W，可能是多余的。
3.为什么最后几个的 W 都一样？
这是因为对于最后的那些，因为步长太大的缘故，导致最后的误差进行震荡，而不能很好地缩减误差。</code></pre><p>另外代码可以在我的 github 中找到。</p>
<p>下面贴一张当 lam = 0.005 迭代 1000 次的图像。</p>
<p><img src="/images/machinelearningalgorithm/2_12.jpg" alt=""></p>
<p>前向逐步回归的主要作用</p>
<pre><code>这种算法的主要作用是可以找出重要的特征，这样就可以停止对不重要特征的搜集。
就像上图中看到的，当迭代到一定次数后，发现有的特征最终趋向于零，所以我们可以放弃对该特征数据的搜集。</code></pre><p>另外，我们应该要知道在应用缩减方法（岭回归和逐步线性回归）时，模型增加了偏差（使得有些特征被放弃），但是也减小了方差（误差  J）。</p>

    </div>

    
    
    
        <div class="reward-container">
  <div>请我喝杯咖啡吧～</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/icon/wechat.jpeg" alt="犀牛 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/icon/aipay.jpeg" alt="犀牛 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
              <a href="/tags/%E5%9B%9E%E5%BD%92/" rel="tag"># 回归</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2018/06/13/thought1/" rel="prev" title="松子开口机背后的原理">
      <i class="fa fa-chevron-left"></i> 松子开口机背后的原理
    </a></div>
      <div class="post-nav-item">
    <a href="/2018/06/14/basis2/" rel="next" title="递归">
      递归 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#理论"><span class="nav-number">1.</span> <span class="nav-text">理论</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#介绍"><span class="nav-number">1.1.</span> <span class="nav-text">介绍</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#原理"><span class="nav-number">1.2.</span> <span class="nav-text">原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#直接求-W"><span class="nav-number">1.2.1.</span> <span class="nav-text">直接求 W</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用梯度下降的方式求-W"><span class="nav-number">1.2.2.</span> <span class="nav-text">用梯度下降的方式求 W</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#代码示例"><span class="nav-number">2.</span> <span class="nav-text">代码示例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#直接求-W-1"><span class="nav-number">2.1.</span> <span class="nav-text">直接求 W</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#代码编辑"><span class="nav-number">2.1.1.</span> <span class="nav-text">代码编辑</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#注意点"><span class="nav-number">2.1.2.</span> <span class="nav-text">注意点</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#欠拟合"><span class="nav-number">3.</span> <span class="nav-text">欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#LWLR"><span class="nav-number">3.1.</span> <span class="nav-text">LWLR</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#原理-1"><span class="nav-number">3.1.1.</span> <span class="nav-text">原理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#看代码解释"><span class="nav-number">3.1.2.</span> <span class="nav-text">看代码解释</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#图像"><span class="nav-number">3.1.3.</span> <span class="nav-text">图像</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#缩减系数来理解数据"><span class="nav-number">4.</span> <span class="nav-text">缩减系数来理解数据</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#什么是缩减系数？"><span class="nav-number">4.1.</span> <span class="nav-text">什么是缩减系数？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#缩减系数的目的"><span class="nav-number">4.2.</span> <span class="nav-text">缩减系数的目的</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#消除噪声特征"><span class="nav-number">4.2.1.</span> <span class="nav-text">消除噪声特征:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#消除关联的特征"><span class="nav-number">4.2.2.</span> <span class="nav-text">消除关联的特征</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#岭回归（ridge-regression）"><span class="nav-number">4.3.</span> <span class="nav-text">岭回归（ridge regression）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#代码示例-1"><span class="nav-number">4.3.1.</span> <span class="nav-text">代码示例</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#前向逐步回归"><span class="nav-number">4.4.</span> <span class="nav-text">前向逐步回归</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#前向逐步回归——贪心算法"><span class="nav-number">4.4.1.</span> <span class="nav-text">前向逐步回归——贪心算法</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="犀牛"
      src="/images/icon/icon.png">
  <p class="site-author-name" itemprop="name">犀牛</p>
  <div class="site-description" itemprop="description">求职web3工作，欢迎联系 xiniublog@163.com</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">2141</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1153</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">627</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="mailto:xiniublog@163.com" title="E-Mail → mailto:xiniublog@163.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="/atom.xml" title="RSS → &#x2F;atom.xml"><i class="fa fa-fw fa-rss"></i>RSS</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">犀牛</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.2
  </div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : 'forest',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'E96BxDaGNGpHB8Fp5JadyD1A-MdYXbMMI',
      appKey     : 'Hp34boCFr2uY72f9aj2V2jgn',
      placeholder: "由于 leancloud 限制，评论只允许海外IP访问。如果你无法通过留言询问问题，请发送邮件到 xiniublog@163.com 。另外，提问问题请遵守 https://benpaodewoniu.github.io/problem/ 。如果你真的想要我联系你或者想交个朋友，请通过邮箱联系我，告知我你的微信号。在评论里留下邮箱，处于岁月静好的原则，我是不会加也不会去打扰的。",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : 'https://e96bxdag.api.lncldglobal.com'
    });
  }, window.Valine);
});
</script>

</body>
</html>
